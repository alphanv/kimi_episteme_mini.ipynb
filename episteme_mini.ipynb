# %% 0. 1-click setup
!pip install -q torch sbi numpyro jax jaxlib pyro-ppl torchdiffeq==0.2.3

import torch, math, numpy as np, matplotlib.pyplot as plt
from torch import nn
from sbi.inference import SNPE
from sbi.utils.posterior_ensemble import NeuralPosteriorEnsemble
device = 'cuda' if torch.cuda.is_available() else 'cpu'

# %% 1. True world: dN/dt = r N (1 - N/K)  (Logistic growth + control)
def true_dynamics(state, t, r, K, u):
    N = state[0]
    dN = r * N * (1 - N/K) + u
    return dN

from scipy.integrate import odeint
def simulate(r=0.8, K=5.0, u=0.0, N0=1.0, T=20, dt=1.0):
    ts = np.arange(0, T, dt)
    sol = odeint(true_dynamics, [N0], ts, args=(r, K, u))
    return torch.tensor(sol.flatten(), dtype=torch.float32)

# %% 2. Agent generative model: Î¸ = [r, K]
class GenerativeModel(nn.Module):
    def __init__(self, N0=1.0, T=20):
        super().__init__()
        self.T, self.dt = T, 1.0
        self.N0 = N0
    def forward(self, theta, action):
        # theta = [r, K]
        r, K = theta[...,0], theta[...,1]
        u = action
        # analytic logistic closed-form under constant control
        t = torch.arange(self.T, dtype=torch.float32)
        L = torch.stack([logistic(r_i, K_i, u, self.N0, t) 
                         for r_i, K_i in zip(r, K)])
        return L + 0.1*torch.randn_like(L)  # observation noise

def logistic(r, K, u, N0, t):
    A = (K + u/r)/N0 - 1
    return K*(1 + A*torch.exp(-(r + u/K)*t))**(-1)

# %% 3. Babelfish encoder: 1-D semantic bottleneck (identity here)
def babelfish(y): return y  # trivial; replace by 32-D CNN later

# %% 4. Episteme loop: active Bayesian inference + experiment design
prior = torch.distributions.Uniform(torch.tensor([0.1, 1.0]),
                                    torch.tensor([2.0, 10.0]))
simulator = GenerativeModel()

def run_episteme(n_rounds=3, n_sim=2000):
    posteriors = []
    actions = [0.0]  # initial action
    for r in range(n_rounds):
        # 1. Train posterior given history
        def sim_wrapper(theta):
            return simulator(theta, torch.tensor(actions[-1]))
        inference = SNPE(prior, density_estimator='maf')
        proposal = prior if r==0 else posteriors[-1]
        theta_train = proposal.sample((n_sim,))
        x_train = torch.stack([sim_wrapper(th) for th in theta_train])
        posterior = inference.append_simulations(theta_train, x_train).train().build_posterior()
        posteriors.append(posterior)
        # 2. Bayesian optimal experiment design: maximize info gain
        candidate_actions = torch.linspace(-1.0, 1.0, 5)
        ig = []
        for a in candidate_actions:
            x_sim = simulator(posterior.sample((500,)), a)
            prior_entropy = posterior.entropy()
            conditional_entropy = posterior.entropy(x=x_sim)
            ig.append(prior_entropy - conditional_entropy)
        next_a = candidate_actions[torch.argmax(torch.tensor(ig))]
        actions.append(float(next_a))
    return posteriors, actions

# %% 5. Demo run
posteriors, actions = run_episteme()
print("Chosen actions:", actions)

# %% 6. Posterior predictive check & symbolic recovery
true = torch.tensor([0.8, 5.0])
samples = posteriors[-1].sample((1000,))
plt.scatter(samples[:,0], samples[:,1], alpha=0.3)
plt.axvline(true[0], color='r'); plt.axhline(true[1], color='r')
plt.xlabel("r"); plt.ylabel("K"); plt.title("Posterior after 3 rounds")
plt.show()

# %% 7. Export symbolic law (least-squares linear fit on log-transform)
from sklearn.linear_model import LinearRegression
t = torch.arange(20, dtype=torch.float32).numpy()
y_obs = simulate(r=0.8, K=5.0, u=actions[-1])
logistic_form = lambda t, r, K: K / (1 + (K-1)/1 * np.exp(-r*t))
popt, _ = torch.lstsq(torch.tensor(y_obs).unsqueeze(1), 
                      torch.vstack([t, t**2]).T)  # crude regression
print("Recovered parameters:", popt.squeeze())